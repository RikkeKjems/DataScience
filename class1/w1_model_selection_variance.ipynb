{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Model selection and (co)variance\n",
    "\n",
    "__Goal__: Learning to turn equations into code\n",
    "\n",
    "\n",
    "## Model selection\n",
    "\n",
    "In your groups, briefly discuss:\n",
    "\n",
    "- What are some methods to do model comparison?\n",
    "- List some pros/cons of each method\n",
    "- What can we do to increase the generalizability of our models?\n",
    "\n",
    "Exercises:\n",
    "- Implement the sum of squares and ridge regression loss functions (Bishop page 10)\n",
    "- How do the loss functions behave as the weights _w_ increase or decrease? \n",
    "- Extra: implement the __lasso__ loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 \n",
    "information criteria, predictive accuracy (incl. loo & waic), fit to data (e.g. R^2). \n",
    "\n",
    "# Q2 \n",
    "predictive accuracy is more robust (against overfitting) as compared to fit (e.g. R^2). \n",
    "Information criteria and other methods (e.g. loo & waic) incorporate this without actually predicting. \n",
    "\n",
    "# Q3\n",
    "ridge regression :) (penalize the number of parameters). \n",
    "more data also good. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy is the go-to library for anything involving vectors \n",
    "# and matrices in Python\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector of true values\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# For simplicity, let's assume we just have a vector of predicted values yhat \n",
    "# instead of expression y(x_n, w) in eq. 1.4\n",
    "yhat = np.array([1, 2, 2.8, 3.7, 5.2, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function calculating the sum of squares\n",
    "def ss(y, yhat):\n",
    "    sum = 0\n",
    "    for i in range(len(y)): \n",
    "        sum += ((yhat[i] - y[i])**2) / 2\n",
    "    return(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08500000000000002"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression loss needs two more parameters, the weights w and \n",
    "# the lambda regularization weight\n",
    "w = np.array([0.5, 1.5]) # Setting two arbitrary weights (betas in a linear regression)\n",
    "lamb = -1 # Arbitrary lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the ridge regression loss function\n",
    "def ridge(y, yhat, w, lamb):\n",
    "    sum = 0\n",
    "    for i in range(len(y)): \n",
    "        sum += (((yhat[i] - y[i])**2) + (lamb/2) * (np.linalg.norm(w)**2)) / 2\n",
    "    return(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of squares: 0.08500000000000002\n",
      "ridge: -3.6650000000000005\n"
     ]
    }
   ],
   "source": [
    "print(f\"sum of squares: {ss(yhat, y)}\")\n",
    "print(f\"ridge: {ridge(yhat, y, w, lamb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge big: -749.9150000000001\n",
      "ridge small: 0.08425000000000002\n",
      "ridge negative lambda: -374999999.915\n",
      "ridge small lambda: 0.08462500000000003\n"
     ]
    }
   ],
   "source": [
    "# What happens to the loss if you make the weights in w larger or smaller?\n",
    "w_big = np.array([10, 20])\n",
    "w_small = np.array([0.01, 0.02])\n",
    "print(f\"ridge big: {ridge(yhat, y, w_big, lamb)}\")\n",
    "print(f\"ridge small: {ridge(yhat, y, w_small, lamb)}\") #getting closer to sum of squares.\n",
    "\n",
    "# What happens if you increase or decrease lambda?\n",
    "lambda_small = -100000000\n",
    "lambda_zero = -0.0001\n",
    "print(f\"ridge negative lambda: {ridge(yhat, y, w, lambda_small)}\") \n",
    "print(f\"ridge small lambda: {ridge(yhat, y, w, lambda_zero)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation, Variance, and covariance\n",
    "\n",
    "Briefly discuss:\n",
    "\n",
    "- What is the intuition behind expectation, variance, and covariance? \n",
    "- What properties do they describe; how do the equations line up with your explanations?\n",
    "- What is the difference between covariance and correlation?\n",
    "- How do you interpret a covariance matrix?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 & Q2: \n",
    "## Expectation: \n",
    "The grand mean, which we hope reflects the population. \n",
    "This is our best guess for any particular value before fitting any kind of model. \n",
    "\n",
    "## Variance: \n",
    "Quantification of how much individual samples vary from the sample mean. \n",
    "The calculation forms the backbone of the standard deviation. \n",
    "\n",
    "## Covariance: \n",
    "Variance between two variables. Useful because we typically don't want variables\n",
    "that covary too much in the same model?\n",
    "\n",
    "# Q3:\n",
    "Covariance: direction of relationship (neg. infinity; pos. infinity). \n",
    "Correllation: strength of relationship (not direction). From (-1, 1). \n",
    "\n",
    "\n",
    "# Q4: \n",
    "blank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises: \n",
    "\n",
    "- Calculate the expectation of the vector _x_ in the code chunk below\n",
    "- Implement functions for calculating variance and covariance (eq. 1.39 and 1.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.12172825138376195\n"
     ]
    }
   ],
   "source": [
    "# sampling 10 random values from a normal distribution with mean 0 and sd 1\n",
    "x = np.random.randn(10)\n",
    "\n",
    "# calculate the expectation of x \n",
    "exp = np.sum(x)/len(x)\n",
    "print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my var: 0.5315716423561934\n",
      "np.var: 0.5315716423561934\n"
     ]
    }
   ],
   "source": [
    "def var(x):\n",
    "    \"Calculate variance following eq. 1.39\"\n",
    "    variance = np.mean((x - np.mean(x)) ** 2)\n",
    "    return(variance)\n",
    "\n",
    "# check if your results match numpy's built in function\n",
    "print(f\"my var: {var(x)}\")\n",
    "print(f\"np.var: {np.var(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my covar: -0.011580242810731788\n",
      "np.cov: -0.011580242810731813\n"
     ]
    }
   ],
   "source": [
    "# sample 10 more random variables\n",
    "y = np.random.randn(10)\n",
    "x = np.random.randn(10)\n",
    "\n",
    "\"Calculate covariance following eq. 1.42\"\n",
    "def covar(x, y):\n",
    "    cov = np.mean((x - np.mean(x)) * (np.transpose(y) - np.sum(np.transpose(y))))\n",
    "    return(cov)\n",
    "\n",
    "# check if result match numpy\n",
    "print(f\"my covar: {covar(x, y)}\")\n",
    "# np.cov calculates the entire covariance matrix; [0][1] extracts the \n",
    "# covariance of the variables\n",
    "print(f\"np.cov: {np.cov(x, y, ddof=0)[0][1]}\") # ddof = 0 (divide by length or length-1)\n",
    "# What does ddoef=0 mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.34973506, -0.01158024],\n",
       "       [-0.01158024,  0.69525929]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How do you interpret the covariance matrix? What does the diagonal describe?\n",
    "np.cov(x, y, ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we care about covariance matrices? \n",
    "- Useful for e.g. calculating and understanding normal distributions! Play around with the first interactive figure in [this link](https://distill.pub/2019/visual-exploration-gaussian-processes/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Extra_: Implement the following two (possibly more intuitive) equations for variance and covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$var(x) =\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^{2}}{n}$$\n",
    "\n",
    "$$cov(x, y) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var2(x): \n",
    "    value = 0\n",
    "    for i in range(len(x)): \n",
    "        value += (x[i] - np.mean(x))**2\n",
    "    new_value = value/len(x)\n",
    "    return(new_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2.9166666666666665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.9166666666666665"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done already? Take a stab at some probability theory exercises from [here][https://www.math.kth.se/matstat/gru/sf1901/TCOMK/exercises.pdf]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
